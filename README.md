# AI safety

## Open letters

• Statement on Superintelligence (2025)

[https://superintelligence-statement.org](https://superintelligence-statement.org)

• Pause Giant AI Experiments: An Open Letter (2023)

[https://futureoflife.org/open-letter/pause-giant-ai-experiments](https://futureoflife.org/open-letter/pause-giant-ai-experiments)

• Statement on AI Risk (2023)

[https://aistatement.com](https://aistatement.com)

## Talks

• *Will AI outsmart human intelligence?* | Geoffrey Hinton (The Royal Institution, 2025)

https://www.youtube.com/watch?v=IkdziSLYzHw

• *The catastrophic risks of AI  — and a safer path* | Yoshua Bengio (TED, 2025)

https://www.youtube.com/watch?v=qe9QSCF-d88

## Books

• *If Anyone Builds It, Everyone Dies* | Eliezer Yudkowsky & Nate Soares (2025)

https://ifanyonebuildsit.com

## Blogs

• Yoshua Bengio

[https://yoshuabengio.org](https://yoshuabengio.org)


## Scientific publications

• Bengio et al. "Managing extreme AI risks amid rapid progress". Science, 384(6698):842–845, 2024.

[https://arxiv.org/pdf/2310.17688](https://arxiv.org/pdf/2310.17688)

## Non-profit organizations

• AI Futures Project

[https://ai-futures.org](https://ai-futures.org)

[https://blog.ai-futures.org](https://blog.ai-futures.org)

[https://ai-2027.com](https://ai-2027.com)

• Forethought

[https://www.forethought.org](https://www.forethought.org)

Center for AI Safety

[https://safe.ai](https://safe.ai)

• LawZero

[https://lawzero.org](https://lawzero.org)

• Centre pour la Sécurité de l'IA

[https://www.securite-ia.fr](https://www.securite-ia.fr)
